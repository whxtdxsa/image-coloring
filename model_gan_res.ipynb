{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_prepare import process_and_split_dataset, load_data\n",
    "\n",
    "if not os.path.exists('./dataset/'):\n",
    "    if not os.path.exists('./CelebAMask-HQ/CelebA-HQ-img/'): \n",
    "        if not os.path.exists('CelebAMask-HQ.zip'):\n",
    "            # Download dataset\n",
    "            !gdown --id 1badu11NqxGf6qM3PTTooQDJvQbejgbTv -O CelebAMask-HQ.zip\n",
    "            \n",
    "        # Unzip dataset\n",
    "        !unzip CelebAMask-HQ.zip 'CelebAMask-HQ/CelebA-HQ-img/*'  \n",
    "\n",
    "    # Split dataset\n",
    "    rawdata_dir = \"./CelebAMask-HQ/CelebA-HQ-img/\"\n",
    "    process_and_split_dataset(rawdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "model_save_path = \"./model/gan-res/\"\n",
    "\n",
    "img_width = 512\n",
    "img_height = 512\n",
    "\n",
    "max_dataset_size = 30000\n",
    "num_epochs = 20\n",
    "\n",
    "lr = 0.0001\n",
    "batch_size = 16\n",
    "\n",
    "lambda_pixel = 100\n",
    "D_update_interval = 3\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Transforms with Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_loader, val_loader, test_loader = load_data(transform, batch_size, max_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        key = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(query, key)\n",
    "        attention = self.softmax(energy)\n",
    "        value = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        return self.gamma * out + x\n",
    "    \n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (64, 256, 256)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (128, 128, 128)\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (256, 64, 64)\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (512, 32, 32)\n",
    "\n",
    "            # 새로운 레이어 추가\n",
    "            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(1024),\n",
    "            ResidualBlock(1024),\n",
    "            ResidualBlock(1024),\n",
    "            ResidualBlock(1024)\n",
    "            # 필요한 만큼의 ResidualBlock을 추가\n",
    "        )\n",
    "        self.attention = SelfAttention(in_dim=1024)  # Attention의 입력 차원 변경\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # size: (256, 64, 64)\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # size: (128, 128, 128)\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # size: (64, 256, 256)\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "            # size: (3, 512, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),  # Dropout 비율 증가\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Dropout 추가\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model, Loss Function, and Optimizer Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_pixelwise = nn.L1Loss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.0004, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.0001, 0.999))\n",
    "\n",
    "def save_model(model, loss, epoch, save_path):\n",
    "    # 모델의 가중치 저장\n",
    "    if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "    generator, discriminator = model\n",
    "    g_loss, d_loss = loss\n",
    "    torch.save(generator.state_dict(), os.path.join(save_path, f'generator_epoch_{epoch+1}_{g_loss}_{d_loss}.pth'))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(save_path, f'discriminator_epoch_{epoch+1}_{g_loss}_{d_loss}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    total_G_loss = 0\n",
    "    total_D_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, (grayscale_imgs, real_imgs) in enumerate(pbar):\n",
    "        grayscale_imgs, real_imgs = grayscale_imgs.to(device), real_imgs.to(device)\n",
    "\n",
    "        # 먼저 fake 이미지를 생성\n",
    "        fake_imgs = generator(grayscale_imgs)\n",
    "\n",
    "        # Discriminator 업데이트 빈도에 따라 조건적으로 업데이트\n",
    "        if batch_idx % D_update_interval == 0:\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real images\n",
    "            real_labels = torch.ones(grayscale_imgs.size(0), 1, device=device)\n",
    "            real_loss = criterion_GAN(discriminator(real_imgs), real_labels)\n",
    "\n",
    "            # Fake images\n",
    "            fake_labels = torch.zeros(grayscale_imgs.size(0), 1, device=device)\n",
    "            fake_loss = criterion_GAN(discriminator(fake_imgs.detach()), fake_labels)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            D_loss = (real_loss + fake_loss) / 2\n",
    "            D_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_labels = torch.ones(grayscale_imgs.size(0), 1, device=device)\n",
    "        GAN_loss = criterion_GAN(discriminator(fake_imgs), fake_labels)\n",
    "\n",
    "        # Pixel-wise loss\n",
    "        pixelwise_loss = criterion_pixelwise(fake_imgs, real_imgs)\n",
    "\n",
    "        # Total generator loss\n",
    "        G_loss = GAN_loss + lambda_pixel * pixelwise_loss\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        total_G_loss += G_loss.item()\n",
    "        total_D_loss += D_loss.item()\n",
    "        pbar.set_postfix(G_loss=G_loss.item(), D_loss=D_loss.item())\n",
    "\n",
    "    avg_G_loss = total_G_loss / len(train_loader)\n",
    "    avg_D_loss = total_D_loss / len(train_loader)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        save_model((generator, discriminator), (avg_G_loss, avg_D_loss), epoch, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "# Visualization of colorization results after each epoch\n",
    "def model_eval(generator, test_loader, device, test_cnt):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            for idx in range(test_cnt):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                fake_imgs = generator(inputs)\n",
    "\n",
    "                # Displaying up to 5 examples\n",
    "                plt.figure(figsize=(12, 4))\n",
    "\n",
    "                # Original Grayscale Image\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(transforms.ToPILImage()(inputs[idx].cpu()), cmap='gray')\n",
    "                plt.title(\"Original Grayscale\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Ground Truth Color Image\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(transforms.ToPILImage()(labels[idx].cpu()))\n",
    "                plt.title(\"Ground Truth Color\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Model's Colorized Image\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(transforms.ToPILImage()(fake_imgs[idx].cpu()))\n",
    "                plt.title(\"Model's Colorized\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.show()\n",
    "model_eval(generator, test_loader, device, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
